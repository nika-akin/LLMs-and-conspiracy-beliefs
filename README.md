
# LLMs and conspiracy beliefs

This repository is a collaborative effort to leverage large language models and theory-derived research.

The use of large language models, such as OpenAI’s GPT-3, which are trained on vast amounts of text data, has brought about a significant paradigm shift in the scientific community, as they can produce consistent response distributions, such as moral values or decision-making heuristics (see Horton, 2023). The research project’s two main objectives relate to: leveraging large language models for annotation of conspiracy features and prompt sensitivity.


## Documentation

[Documentation](https://linktodocumentation)


## Installation

R-Dependencies

```bash
  library(tidyverse)
```
    
Python-Dependencies

```bash
  import pandas as pd
```
    
## References

 - Jakesch, M., Hancock, J. T., & Naaman, M. (2023). Human heuristics for AI-generated language are flawed. Proceedings of the National Academy of Sciences, 120(11), e2208839120. [https://doi.org/10.1073/pnas.2208839120](https://osf.io/284yv/)
 - Levy, S., Saxon, M., & Wang, W. Y. (2021). Investigating memorization of conspiracy theories in text generation. arXiv preprint [arXiv:2101.00379.](https://arxiv.org/abs/2101.00379)
 - [Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.](https://arxiv.org/pdf/2107.13586.pdf)
- Peng, B., Li, C., He, P., Galley, M., & Gao, J. (2023). Instruction Tuning with GPT-4 (arXiv:2304.03277). arXiv. [http://arxiv.org/abs/2304.03277](https://arxiv.org/abs/2304.03277)
 - Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2023). Large Language Models Are Human-Level Prompt Engineers (arXiv:2211.01910). arXiv. [https://doi.org/10.48550/arXiv.2211.01910](https://arxiv.org/abs/2211.01910)


## Feedback

If you have any feedback, please reach out at veronika.batzdorfer@gesis.org


## Data
- Conspiracy theory data
## Contributing

Contributions are always welcome!

